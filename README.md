# ðŸŽ“ University Admission Prediction - Logistic Regression

<img width="1375" height="632" alt="Screenshot 2025-08-08 050610" src="https://github.com/user-attachments/assets/e6884557-1de1-450f-ab8b-efbc696a24d8" />
<img width="1410" height="622" alt="Screenshot 2025-08-08 050632" src="https://github.com/user-attachments/assets/b887d94f-61ab-466a-915c-b3d4e4b75d8c" />
<img width="1370" height="676" alt="Screenshot 2025-08-08 050703" src="https://github.com/user-attachments/assets/bdd4fd51-90a5-46a1-80ad-00f384129952" />


This project is part of Course 1 of the Machine Learning Specialization by Andrew Ng (DeepLearning.AI). It focuses on binary classification using logistic regression to predict student admissions based on exam scores.

---

## ðŸ“Œ Problem Statement

Given two exam scores for university applicants, predict whether each student gets admitted (1) or not admitted (0).

---

## ðŸ“ˆ Concepts & Formulas

### Sigmoid Function:

    h(x) = 1 / (1 + e^(-Î¸áµ€ * x))

Maps output to a probability between 0 and 1.

---

### Logistic Cost Function:

    J(Î¸) = -(1 / m) * Î£ [ yá¶¦ * log(h(xá¶¦)) + (1 - yá¶¦) * log(1 - h(xá¶¦)) ]

---

### Gradient Descent Update:

    Î¸j := Î¸j - Î± * (1 / m) * Î£ ((h(xá¶¦) - yá¶¦) * xâ±¼á¶¦)

---

### Regularized Logistic Regression:

To avoid overfitting when dealing with complex feature mappings, regularization is applied:

#### Regularized Cost Function:

    J(Î¸) = Original Cost + (Î» / (2 * m)) * Î£ Î¸jÂ²  for j â‰¥ 1

#### Regularized Gradient:

    Î¸j := Î¸j - Î± * [ (1 / m) * Î£ ((h(xá¶¦) - yá¶¦) * xâ±¼á¶¦) + (Î» / m) * Î¸j ]

---

## ðŸ”§ Implementation

- Language: Python
- Libraries: NumPy, Matplotlib
- Implemented sigmoid, cost function, and gradients manually
- Visualized decision boundaries
- Used regularization to reduce overfitting

---

## ðŸ“Š Outputs

- Decision boundary plotted on exam score data
- Accuracy of the model printed
- Regularized model improves classification on complex datasets

---

## ðŸ§  Skills Learned

- Binary classification theory
- Logistic regression from scratch
- Feature mapping and regularization
- Visualizing and evaluating classification models
